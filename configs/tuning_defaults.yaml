# Baseline configuration used when sweeping hyperparameters.
expt_name: tuning_sweep
arch: resnet1d
loader: task_incremental_loader

data_path: data/rff/radar_det_cls/hpt
dataset: iq
data_scaling: normalize
shuffle_tasks: false
classes_per_it: 6
iterations: 5000
workers: 0
validation: 0.3
class_order: random
increment: 5
test_batch_size: 128
samples_per_task: -1
val_rate: 3

cuda: true
seed: 0
log_every: 50
log_dir: logs/tuning/individual
tf_dir: ""
calc_test_accuracy: false
state_logging: false

glances: 1
n_meta: 1
inner_steps: 1
temperature: 5
n_epochs: 5
optimizer: sgd
batch_size: 128
# replay_batch_size: 64
# memories: 1024
# grad_clip_norm: 5.0
# lr: 0.03

# La-MAML parameters
opt_lr: 0.1                          # learning rate for LRs
opt_wt: 0.1                          # learning rate for weights
alpha_init: 0.1                    # initialization for the LRs
learn_lr: false                      # model should update the LRs during learning
sync_update: false                   # the LRs and weights should be updated synchronously
cifar_batches: 3                     # Number of batches in inner trajectory
use_old_task_memory: false           # Use only old task samples for replay buffer data
second_order: true                  # use second order MAML updates

# Memory parameters
n_memories: 5120                        # number of memories per task
memory_strength: 1.0                 # memory strength (meaning depends on memory)
steps_per_sample: 1                  # training steps per batch

# Parameters specific to MER
gamma: 1.0                           # gamma learning rate parameter
beta: 0.1                            # beta learning rate parameter
s: 1.0                               # current example learning rate multiplier (s)
batches_per_example: 1.0             # the number of batch per incoming example

# Parameters specific to Meta-BGD
bgd_optimizer: bgd                   # Optimizer (choices: [adam, adagrad, bgd, sgd])
optimizer_params: "{}"               # Optimizer parameters
train_mc_iters: 2                    # Number of MonteCarlo samples during training (default 10)
std_init: 0.02                       # STD init value (default 5e-2)
mean_eta: 50.                        # Eta for mean step (default 1)
fisher_gamma: 0.95                   # Fisher gamma

# Parameters specific to ANML
rln: 7
update_steps: 10
meta_lr: 0.001 
update_lr: 0.1 
# steps: 20000

# Parameters specific to CTN
beta: 0.05                        # Beta parameter for CTN
temperature: 5
alpha_init: 0.1
task_emb: 64

# Parameters specific to BCL