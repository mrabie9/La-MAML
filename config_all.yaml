# Experiment details
expt_name: bcl_basic_test        # name of the experiment
model: bcl_dual                        # algo to train
arch: resnet1d                          # arch to use for training (choices: [linear, pc_cnn, resnet18, resnet1d])
# n_hiddens: 100                          # number of hidden neurons at each layer
# n_layers: 2                             # number of hidden layers
xav_init: true                          # Use xavier initialization

# Data parameters
data_path: data/rff/rfmls            # path where data is located
loader: task_incremental_loader      # data loader to use
shuffle_tasks: false                 # NOT USED present tasks in order
classes_per_it: 5                    # number of classes in every batch
iterations: 5000                     # number of classes in every batch
dataset: iq                 # Dataset to train and test on
workers: 0                           # Number of workers preprocessing the data
validation: 0.2                      # Validation split (0. <= x <= 1.)
class_order: random                     # define classes order of increment (choices: [random, chrono, old, super])
increment: 5                         # number of classes to increment by in class incremental loader
test_batch_size: 100000              # batch size to use during testing

# Experiment parameters
cuda: true                    # Use GPU
seed: 0                       # random seed of model
log_every: 3125                # frequency of checking the validation accuracy, in minibatches
log_dir: logs/                # the directory where the logs will be saved
tf_dir: ""                    # (not set by user)
calc_test_accuracy: false     # Calculate test accuracy along with val accuracy

# Optimizer parameters (influencing all models)
glances: 1                    # Number of times the model is allowed to train over a set of samples in the single pass setting
n_epochs: 20                   # Number of epochs per task
batch_size: 64                 # the amount of items received by the algorithm at one time (set to 1 across all experiments). Variable name is from GEM project.
replay_batch_size: 10         # The batch size for experience replay
samples_per_task: 512                 # training samples per task (all if negative)
memories: 400                # number of total memories stored in a reservoir sampling based buffer
grad_clip_norm: 5.0                  # Clip the gradients by this value
lr: 0.03                     # learning rate (For baselines)

# La-MAML parameters
opt_lr: 0.1                          # learning rate for LRs
opt_wt: 0.1                          # learning rate for weights
alpha_init: 0.1                    # initialization for the LRs
learn_lr: false                      # model should update the LRs during learning
sync_update: false                   # the LRs and weights should be updated synchronously
cifar_batches: 3                     # Number of batches in inner trajectory
use_old_task_memory: false           # Use only old task samples for replay buffer data
second_order: true                  # use second order MAML updates

# Memory parameters for GEM | AGEM | ICARL
n_memories: 200                        # number of memories per task
memory_strength: 1.0                 # memory strength (meaning depends on memory)
steps_per_sample: 1                  # training steps per batch

# Parameters specific to MER
gamma: 1.0                           # gamma learning rate parameter
beta: 0.1                            # beta learning rate parameter
s: 1.0                               # current example learning rate multiplier (s)
batches_per_example: 1.0             # the number of batch per incoming example

# Parameters specific to Meta-BGD
bgd_optimizer: bgd                   # Optimizer (choices: [adam, adagrad, bgd, sgd])
optimizer_params: "{}"               # Optimizer parameters
train_mc_iters: 2                    # Number of MonteCarlo samples during training (default 10)
std_init: 0.02                       # STD init value (default 5e-2)
mean_eta: 50.                        # Eta for mean step (default 1)
fisher_gamma: 0.95                   # Fisher gamma

# Parameters specific to ANML
rln: 7
update_steps: 10
meta_lr: 0.001 
update_lr: 0.1 
# steps: 20000

# Parameters specific to CTN
ctn_lr: 0.01
ctn_beta: 0.05                        # Beta parameter for CTN
ctn_n_meta: 2
ctn_inner_steps: 2
ctn_temperature: 5
ctn_n_memories: 50  
ctn_alpha_init: 0.1
ctn_memory_strength: 100
ctn_task_emb: 64

# Parameters specific to BCL
# bcl_adapt: 