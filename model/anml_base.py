# TODO: update to work with ResNet PLN

import logging
import matplotlib.pyplot as plt

import numpy as np
import torch
from torch import nn
from torch.nn import functional as F
from model.resnet1d import ResNet1D
from model.modelfactory import ModelFactory
# from dataloaders.iq_data_loader import ensure_iq_two_channel

logger = logging.getLogger("experiment")

def batchnorm(input, weight=None, bias=None, running_mean=None, running_var=None, training=True, eps=1e-5, momentum=0.1):
    ''' momentum = 1 restricts stats to the current mini-batch '''
    # This hack only works when momentum is 1 and avoids needing to track running stats
    # by substuting dummy variables
    running_mean = torch.zeros(np.prod(np.array(input.data.size()[1])))
    running_var = torch.ones(np.prod(np.array(input.data.size()[1])))
    return F.batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)

def maxpool(input, kernel_size, stride=None):
    return F.max_pool2d(input, kernel_size, stride)
def maxpool1d(input, kernel_size, stride=None):
    return F.max_pool1d(input, kernel_size, stride)

def conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):
    return F.conv2d(input, weight, bias, stride, padding, dilation, groups)
def conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):
    return F.conv1d(input, weight, bias, stride, padding, dilation, groups)

class Learner(nn.Module):

    def __init__(self, n_outputs, args, neuromodulation=True):
        """
        :param config: network config file, type:list of (string, list)
        :param imgc: 1 or 3
        :param imgsz:  28 or 84
        """
        super(Learner, self).__init__()
        self.args = args
        
        # self.config = config
        self.Neuromodulation = neuromodulation
        # this dict contains all tensors needed to be optimized
        self.vars = nn.ParameterList()
        # running_mean and running_var
        self.vars_bn = nn.ParameterList()
        self.net = ResNet1D(n_outputs, self.args)
        self.net.define_task_lr_params(alpha_init=self.args.alpha_init)

        self.config = ModelFactory.get_model()

        for i, (name, param) in enumerate(self.config):
            if 'conv' in name:
                cout, cin, k = param[:3]       # ensure your config stores 1D conv as [cout, cin, k, (stride), (pad)]
                w = nn.Parameter(torch.empty(cout, cin, k))
                torch.nn.init.kaiming_normal_(w)
                # [ch_out, ch_in, kernelsz]
                self.vars.append(w)
                # [ch_out]
                self.vars.append(nn.Parameter(torch.zeros(cout)))

            elif 'linear' in name or 'nm_to' in name or name == 'fc':

                # [ch_out, ch_in]
                sz_r, sz_int = param
                w = nn.Parameter(torch.ones(*param))
                torch.nn.init.kaiming_normal_(w)
                self.vars.append(w)
                self.vars.append(nn.Parameter(torch.zeros(param[0])))

            elif 'bn' in name:
                # [ch_out]
                w = nn.Parameter(torch.ones(param[0]))
                self.vars.append(w)
                # [ch_out]
                self.vars.append(nn.Parameter(torch.zeros(param[0])))

                # must set requires_grad=False
                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)
                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)
                self.vars_bn.extend([running_mean, running_var])
                
            else:
                raise NotImplementedError

        # # iterate over all parameters
        # for name, param in self.model.named_parameters():
        #     self.vars.append(param)

        # # iterate over buffers (like BN running stats)
        # for name, buf in self.model.named_buffers():
        #     if "running_mean" in name or "running_var" in name:
        #         # match your old code: store them but freeze grads
        #         p = nn.Parameter(buf.clone().detach(), requires_grad=False)
        #         self.vars_bn.append(p)


    def forward(self, x, vars=None, bn_training=True, feature=False):
        """
        This function can be called by finetunning, however, in finetunning, we dont wish to update
        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.
        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False
        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.
        :param x: [b, 1, 28, 28]
        :param vars:
        :param bn_training: set False to not update
        :return: x, loss, likelihood, kld
        """
        
        cat_var = False
        cat_list = []

        if vars is None:
            vars = self.vars
        idx = 0
        bn_idx = 0

        if self.Neuromodulation:

            # =========== NEUROMODULATORY NETWORK ===========

            #'conv1_nm'
            #'bn1_nm'
            #'conv2_nm'
            #'bn2_nm'
            #'conv3_nm'
            #'bn3_nm'

          
            # Query the neuromodulatory network:
            
            
            data = x.view(-1,2,2048)
            nm_data = x.view(-1,2,2048)

            #input_mask = self.call_input_nm(data_, vars)
            #fc_mask = self.call_fc_nm(data_, vars)

            w,b = vars[0], vars[1]
            nm_data = conv1d(nm_data, w, b)
            w,b = vars[2], vars[3]
            running_mean, running_var = self.vars_bn[0], self.vars_bn[1]
            nm_data = F.batch_norm(nm_data, running_mean, running_var, weight=w, bias=b, training=True)

            nm_data = F.relu(nm_data)
            nm_data = maxpool1d(nm_data, kernel_size=2, stride=2)

            w,b = vars[4], vars[5]
            nm_data = conv1d(nm_data, w, b)
            w,b = vars[6], vars[7]
            running_mean, running_var = self.vars_bn[2], self.vars_bn[3]
            nm_data = F.batch_norm(nm_data, running_mean, running_var, weight=w, bias=b, training=True)

            nm_data = F.relu(nm_data)
            nm_data = maxpool1d(nm_data, kernel_size=2, stride=2)

            w,b = vars[8], vars[9]
            nm_data = conv1d(nm_data, w, b)
            w,b = vars[10], vars[11]
            running_mean, running_var = self.vars_bn[4], self.vars_bn[5]
            nm_data = F.batch_norm(nm_data, running_mean, running_var, weight=w, bias=b, training=True)
            nm_data = F.relu(nm_data)
            #nm_data = maxpool(nm_data, kernel_size=2, stride=2)


            nm_data = nm_data.view(nm_data.size(0), -1)

            # NM Output

            w = vars[12]
            b = vars[13]
            fc_mask = F.sigmoid(F.linear(nm_data, w, b))#.view(nm_data.size(0), 512)


            # =========== PREDICTION NETWORK ===========

            prediction = self.net.forward(x, fc_mask)

            try:
                prediction = torch.cat([prediction, data], dim=0)
            except:
                prediction = data
            
            return(prediction)


        else:
            x = self.net.forward(x)
            return (x) 

    def zero_grad(self, vars=None):
        """
        :param vars:
        :return:
        """
        with torch.no_grad():
            if vars is None:
                for p in self.vars:
                    if p.grad is not None:
                        p.grad.zero_()
            else:
                for p in vars:
                    if p.grad is not None:
                        p.grad.zero_()

    def parameters(self):
        """
        override this function since initial parameters will return with a generator.
        :return:
        """
        return self.vars
    

import torch
import torch.nn as nn
import torch.nn.functional as F

class NeuromodNet1D(nn.Module):
    """
    A 1D neuromodulatory network that outputs a per-sample mask.
    Compatible with MAML/ANML: can accept a dict of fast-weights (fw).
    """
    def __init__(self, in_ch: int, mask_dim: int, conv_ch=(64, 112, 112),
                 k=5, pool_every=(1,), stride=1, pad='same'):
        super().__init__()
        C1, C2, C3 = conv_ch

        # --- Conv stack (registered so you can outer-train them) ---
        self.conv1 = nn.Conv1d(in_ch, C1, kernel_size=k, stride=stride,
                               padding=(k//2 if pad=='same' else 0), bias=True)
        self.bn1   = nn.BatchNorm1d(C1, affine=True)

        self.conv2 = nn.Conv1d(C1, C2, kernel_size=k, stride=stride,
                               padding=(k//2 if pad=='same' else 0), bias=True)
        self.bn2   = nn.BatchNorm1d(C2, affine=True)

        self.conv3 = nn.Conv1d(C2, C3, kernel_size=k, stride=stride,
                               padding=(k//2 if pad=='same' else 0), bias=True)
        self.bn3   = nn.BatchNorm1d(C3, affine=True)

        self.pool_every = set(pool_every)  # layers after which to pool (e.g., {2})
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)

        # --- Weâ€™ll set fc after we know in_features via dummy pass ---
        self.fc = None
        self.mask_dim = mask_dim

    # helpers to pick fast-weights if provided
    def _W(self, module, attr, fw):
        if fw is None: return getattr(module, attr)
        key = f'{module._get_name()}.{id(module)}.{attr}'
        return fw.get(key, getattr(module, attr))

    def _bn(self, bn_mod, x, fw):
        w = self._W(bn_mod, 'weight', fw)
        b = self._W(bn_mod, 'bias', fw)
        # use running stats from module (fast BN stats are rarely needed)
        return F.batch_norm(x, bn_mod.running_mean, bn_mod.running_var,
                            weight=w, bias=b, training=self.training)

    def features_only(self, x, fw=None):
        # x: (B, Cin, L)
        x = F.conv1d(x, self._W(self.conv1, 'weight', fw), self._W(self.conv1, 'bias', fw),
                     stride=self.conv1.stride, padding=self.conv1.padding, dilation=self.conv1.dilation, groups=1)
        x = self._bn(self.bn1, x, fw); x = F.relu(x)
        if 1 in self.pool_every: x = self.pool(x)

        x = F.conv1d(x, self._W(self.conv2, 'weight', fw), self._W(self.conv2, 'bias', fw),
                     stride=self.conv2.stride, padding=self.conv2.padding, dilation=self.conv2.dilation, groups=1)
        x = self._bn(self.bn2, x, fw); x = F.relu(x)
        if 2 in self.pool_every: x = self.pool(x)

        x = F.conv1d(x, self._W(self.conv3, 'weight', fw), self._W(self.conv3, 'bias', fw),
                     stride=self.conv3.stride, padding=self.conv3.padding, dilation=self.conv3.dilation, groups=1)
        x = self._bn(self.bn3, x, fw); x = F.relu(x)
        if 3 in self.pool_every: x = self.pool(x)

        return x  # (B, C3, L_out)

    def init_fc_if_needed(self, x_sample):
        if self.fc is not None: return
        with torch.no_grad():
            feats = self.features_only(x_sample)           # (1, C3, L_out)
            in_feat = feats.view(1, -1).size(1)
        self.fc = nn.Linear(in_feat, self.mask_dim, device=x_sample.device)

    def forward(self, x, fw=None):
        """
        Returns mask in (B, mask_dim), sigmoid in [0,1].
        If fw is provided (dict), uses those tensors as fast-weights.
        """
        if self.fc is None:
            # one-sample dummy to infer fc in_features
            dummy = x[:1]
            self.init_fc_if_needed(dummy)

        feats = self.features_only(x, fw)           # (B, C3, Lout)
        flat  = feats.view(feats.size(0), -1)
        W = self._W(self.fc, 'weight', fw) if fw else self.fc.weight
        b = self._W(self.fc, 'bias', fw) if fw else self.fc.bias
        mask = torch.sigmoid(F.linear(flat, W, b))  # (B, mask_dim)
        return mask
