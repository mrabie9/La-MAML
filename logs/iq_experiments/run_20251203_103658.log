python3 main.py $IQ --model agem --expt_name all_agem --n_memories 200 --batch_size 128 --n_epochs 1 
                    --lr 0.03 --glances 1 --memory_strength 0.5   --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model eralg4 --expt_name all_eralg4 --memories 200 --batch_size 128 --n_epochs 1 --replay_batch_size 64 
                     --lr 0.03 --glances 1   --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model eralg4 --expt_name all_la-eralg4 --memories 200 --batch_size 128 --replay_batch_size 64 --n_epochs 1 
                    --opt_lr 0.1 --alpha_init 0.1 --glances 1  --increment 5 
                     --cifar_batches 5 --learn_lr --log_every 3125 --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model gem --expt_name all_gem --n_memories 200 --batch_size 128 --n_epochs 1 
                    --lr 0.03 --glances 1 --memory_strength 0.5   --increment 5 
                    --log_every 3125 --class_order random --samples_per_task 2500 
                    --seed $SEED --grad_clip_norm 5.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model meralg1 --expt_name all_meralg1 --batch_size 128 --memories 200 --replay_batch_size 64 
                    --lr 0.1 --beta 0.1 --gamma 1.0 --batches_per_example 10  --increment 5 
                    --log_every 3125 --grad_clip_norm 10.0 --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model ewc --expt_name all_ewc --batch_size 128 --n_epochs 1 
                    --lr 0.03 --glances 1 --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --grad_clip_norm 25.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model lwf --expt_name all_lwf --batch_size 128 --n_epochs 1 
                    --lr 0.001 --glances 1 --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --grad_clip_norm 5.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model packnet --expt_name all_packnet --batch_size 128 --n_epochs 1 
                    --lr 0.01 --glances 1 --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --grad_clip_norm 5.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model rwalk --expt_name all_rwalk --batch_size 128 --n_epochs 1 
                    --lr 0.001 --glances 1 --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --grad_clip_norm 25.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model si --expt_name all_si --batch_size 128 --n_epochs 1 
                    --lr 0.001 --glances 1 --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --grad_clip_norm 25.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model hat --expt_name all_hat --batch_size 128 --n_epochs 1 
                    --lr 0.0005 --gamma 0.75 --glances 1 --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --grad_clip_norm 10.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model ctn --expt_name all_ctn --memories 200 --batch_size 128 --replay_batch_size 64 --n_epochs 1 
                    --lr 0.03 --glances 1 --increment 5 
                    --ctn_n_memories 200 --ctn_lr 0.01 --ctn_beta 0.05 --ctn_inner_steps 2 --ctn_n_meta 2 
                    --log_every 3125 --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model er_ring --expt_name all_erring --memories 200 --batch_size 128 --replay_batch_size 64 --n_epochs 1 
                    --lr 0.03 --glances 1 --increment 5 
                    --bcl_n_memories 200 --bcl_temperature 2.0 --bcl_memory_strength 1.0 --bcl_inner_steps 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model ucl_bresnet --expt_name all_ucl_bresnet --batch_size 64 --n_epochs 1 
                    --lr 0.001 --glances 1 --increment 5 
                    --log_every 3125 --class_order random 
                    --seed $SEED --grad_clip_norm 10.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model meta-bgd --expt_name all_meta-bgd --memories 200 --batch_size 128 --replay_batch_size 64 --n_epochs 1 
                    --alpha_init 0.1 --glances 1  --increment 5 
                    --cifar_batches 3 --log_every 3125 --second_order --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128 --xav_init  --std_init 0.02 --mean_eta 50. --train_mc_iters 2

python3 main.py $IQ --model lamaml_cifar --expt_name all_lamaml --memories 200 --batch_size 128 --replay_batch_size 64 --n_epochs 1 
                    --opt_lr 0.25 --alpha_init 0.1 --opt_wt 0.1 --glances 1  --increment 5 
                    --cifar_batches 5 --learn_lr --log_every 3125 --second_order --class_order random 
                    --seed $SEED --grad_clip_norm 1.0 --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model lamaml_cifar --expt_name all_lamaml_sync --memories 200 --batch_size 128 --replay_batch_size 64 --n_epochs 1 
                    --opt_lr 0.35 --alpha_init 0.1 --opt_wt 0.1 --glances 1  --increment 5 
                    --cifar_batches 5 --learn_lr --sync_update --log_every 3125 --second_order --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

python3 main.py $IQ --model lamaml_cifar --expt_name all_cmaml --memories 200 --batch_size 128 --replay_batch_size 64 --n_epochs 1 
                    --opt_lr 0.35 --alpha_init 0.075 --opt_wt 0.075 --glances 1  --increment 5 
                    --cifar_batches 5 --sync_update --log_every 3125 --second_order --class_order random 
                    --seed $SEED --calc_test_accuracy --validation 0.3 --samples_per_task 128

Logging IQ experiment suite to logs/iq_experiments/run_20251203_103658.log
New Experiment Starting...
Running model:  agem
[STATE 2025-12-03 10:37:00] Experiment 'all_agem' starting with model 'agem' (seed 0)
Set seed 0
Loaded task0.npz: Unique train labels: [2 3 5 7 8]
Loaded task0.npz: Remapped labels: [0 1 2 3 4]
Loaded task1.npz: Unique train labels: [0 1 4 6 9]
Loaded task1.npz: Remapped labels: [5 6 7 8 9]
[STATE 2025-12-03 10:37:02] Loader 'task_incremental_loader' ready: 4096 inputs, 10 outputs, 2 tasks
n_outputs: 10 	n_tasks: 2
[STATE 2025-12-03 10:37:02] Logging to logs//agem/all_agem-2025-12-03_10-37-02-3549/0
True
Model device: cuda:0
[STATE 2025-12-03 10:37:02] Model initialized on device cuda:0
[STATE 2025-12-03 10:37:02] Invoking continual life experience flow
[STATE 2025-12-03 10:37:02] Life experience start: 2 tasks queued
[STATE 2025-12-03 10:37:02] Starting task 0 (1/2)
[STATE 2025-12-03 10:37:02] Task 0 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:37:02] Task 0 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.046825396825396826, 0.15912698412698412] ----
Task 0 Epoch 1/1 | Loss 1.7264 | Train Acc 0.2422 | Val Acc 0.0468 | Epoch Time 1.89s (Eval 1.69s, Train 0.20s)
[STATE 2025-12-03 10:37:04] Task 0 Epoch 1/1 complete: 1.89s total (1.69s eval/0.20s train)
[STATE 2025-12-03 10:37:04] Task 0: running final validation.
[STATE 2025-12-03 10:37:09] Completed task 0 (1/2)
[STATE 2025-12-03 10:37:09] Starting task 1 (2/2)
[STATE 2025-12-03 10:37:09] Task 1 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:37:09] Task 1 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.046031746031746035, 0.16111111111111112] ----
Task 1 Epoch 1/1 | Loss 1.6514 | Train Acc 0.2109 | Val Acc 0.1611 | Epoch Time 1.67s (Eval 1.56s, Train 0.11s)
[STATE 2025-12-03 10:37:10] Task 1 Epoch 1/1 complete: 1.67s total (1.56s eval/0.11s train)
[STATE 2025-12-03 10:37:10] Task 1: running final validation.
[STATE 2025-12-03 10:37:15] Completed task 1 (2/2)
####Final Validation Accuracy####
Final Results:- 
 Total Accuracy: 0.11726190476190476 
 Individual Accuracy: [0.08452380952380953, 0.15]
####Final Test Accuracy####
Final Results:- 
 Total Accuracy: 0.11726190476190476 
 Individual Accuracy: [0.08452380952380953, 0.15]
[STATE 2025-12-03 10:37:15] Saving results to logs//agem/all_agem-2025-12-03_10-37-02-3549/0/results
logs//agem/all_agem-2025-12-03_10-37-02-3549/0/results: {'expt_name': 'all_agem', 'model': 'agem', 'arch': 'resnet1d', 'n_hiddens': 100, 'n_layers': 2, 'xav_init': False, 'glances': 1, 'n_epochs': 1, 'batch_size': 128, 'replay_batch_size': 20, 'memories': 5120, 'lr': 0.03, 'cuda': True, 'seed': 0, 'log_every': 3125, 'log_dir': 'logs//agem/all_agem-2025-12-03_10-37-02-3549/0', 'tf_dir': 'logs//agem/all_agem-2025-12-03_10-37-02-3549/0/tfdir', 'calc_test_accuracy': True, 'state_logging': True, 'data_path': 'data/rff/rfmls/', 'loader': 'task_incremental_loader', 'samples_per_task': 128, 'shuffle_tasks': False, 'classes_per_it': 4, 'iterations': 5000, 'dataset': 'iq', 'workers': 3, 'validation': 0.3, 'class_order': 'random', 'increment': 5, 'test_batch_size': 100000, 'nc_per_task': 5, 'val_rate': 10, 'opt_lr': 0.1, 'opt_wt': 0.1, 'alpha_init': 0.001, 'learn_lr': False, 'sync_update': False, 'grad_clip_norm': 2.0, 'cifar_batches': 3, 'use_old_task_memory': False, 'second_order': False, 'n_memories': 200, 'memory_strength': 0.5, 'steps_per_sample': 1, 'gamma': 1.0, 'beta': 1.0, 's': 1, 'batches_per_example': 1, 'bgd_optimizer': 'bgd', 'optimizer_params': '{}', 'train_mc_iters': 5, 'std_init': 0.05, 'mean_eta': 1, 'fisher_gamma': 0.95, 'rln': 7, 'update_steps': 10, 'meta_lr': 0.001, 'update_lr': 0.1, 'ctn_lr': 0.01, 'ctn_beta': 0.05, 'ctn_n_meta': 2, 'ctn_inner_steps': 2, 'ctn_temperature': 5, 'ctn_n_memories': 50, 'ctn_alpha_init': 0.1, 'ctn_memory_strength': 0.5, 'ctn_task_emb': 64, 'bcl_n_memories': 2000, 'bcl_memory_strength': 1.0, 'bcl_temperature': 2.0, 'bcl_inner_steps': 5, 'bcl_n_meta': 5, 'bcl_adapt_lr': 0.1} # val: 0.098 0.117 0.019 0.001 # test: 0.098 0.117 0.019 0.000 # 12.901588678359985
[STATE 2025-12-03 10:37:15] Results saved; total runtime 12.90s
New Experiment Starting...
Running model:  eralg4
[STATE 2025-12-03 10:37:18] Experiment 'all_eralg4' starting with model 'eralg4' (seed 0)
Set seed 0
Loaded task0.npz: Unique train labels: [2 3 5 7 8]
Loaded task0.npz: Remapped labels: [0 1 2 3 4]
Loaded task1.npz: Unique train labels: [0 1 4 6 9]
Loaded task1.npz: Remapped labels: [5 6 7 8 9]
[STATE 2025-12-03 10:37:20] Loader 'task_incremental_loader' ready: 4096 inputs, 10 outputs, 2 tasks
n_outputs: 10 	n_tasks: 2
[STATE 2025-12-03 10:37:20] Logging to logs//eralg4/all_eralg4-2025-12-03_10-37-20-2449/0
True
Model device: cuda:0
[STATE 2025-12-03 10:37:20] Model initialized on device cuda:0
[STATE 2025-12-03 10:37:20] Invoking continual life experience flow
[STATE 2025-12-03 10:37:20] Life experience start: 2 tasks queued
[STATE 2025-12-03 10:37:20] Starting task 0 (1/2)
[STATE 2025-12-03 10:37:20] Task 0 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:37:20] Task 0 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.20436507936507936, 0.1988095238095238] ----
Task 0 Epoch 1/1 | Loss 1.7264 | Train Acc 0.2422 | Val Acc 0.2044 | Epoch Time 1.90s (Eval 1.68s, Train 0.22s)
[STATE 2025-12-03 10:37:22] Task 0 Epoch 1/1 complete: 1.90s total (1.68s eval/0.22s train)
[STATE 2025-12-03 10:37:22] Task 0: running final validation.
[STATE 2025-12-03 10:37:26] Completed task 0 (1/2)
[STATE 2025-12-03 10:37:26] Starting task 1 (2/2)
[STATE 2025-12-03 10:37:26] Task 1 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:37:26] Task 1 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.2361111111111111, 0.2015873015873016] ----
Task 1 Epoch 1/1 | Loss 1.5860 | Train Acc 0.2552 | Val Acc 0.2016 | Epoch Time 1.77s (Eval 1.55s, Train 0.22s)
[STATE 2025-12-03 10:37:28] Task 1 Epoch 1/1 complete: 1.77s total (1.55s eval/0.22s train)
[STATE 2025-12-03 10:37:28] Task 1: running final validation.
[STATE 2025-12-03 10:37:33] Completed task 1 (2/2)
####Final Validation Accuracy####
Final Results:- 
 Total Accuracy: 0.2283730158730159 
 Individual Accuracy: [0.2496031746031746, 0.20714285714285716]
####Final Test Accuracy####
Final Results:- 
 Total Accuracy: 0.2283730158730159 
 Individual Accuracy: [0.2496031746031746, 0.20714285714285716]
[STATE 2025-12-03 10:37:33] Saving results to logs//eralg4/all_eralg4-2025-12-03_10-37-20-2449/0/results
logs//eralg4/all_eralg4-2025-12-03_10-37-20-2449/0/results: {'expt_name': 'all_eralg4', 'model': 'eralg4', 'arch': 'resnet1d', 'n_hiddens': 100, 'n_layers': 2, 'xav_init': False, 'glances': 1, 'n_epochs': 1, 'batch_size': 128, 'replay_batch_size': 64.0, 'memories': 200, 'lr': 0.03, 'cuda': True, 'seed': 0, 'log_every': 3125, 'log_dir': 'logs//eralg4/all_eralg4-2025-12-03_10-37-20-2449/0', 'tf_dir': 'logs//eralg4/all_eralg4-2025-12-03_10-37-20-2449/0/tfdir', 'calc_test_accuracy': True, 'state_logging': True, 'data_path': 'data/rff/rfmls/', 'loader': 'task_incremental_loader', 'samples_per_task': 128, 'shuffle_tasks': False, 'classes_per_it': 4, 'iterations': 5000, 'dataset': 'iq', 'workers': 3, 'validation': 0.3, 'class_order': 'random', 'increment': 5, 'test_batch_size': 100000, 'nc_per_task': 5, 'val_rate': 10, 'opt_lr': 0.1, 'opt_wt': 0.1, 'alpha_init': 0.001, 'learn_lr': False, 'sync_update': False, 'grad_clip_norm': 2.0, 'cifar_batches': 3, 'use_old_task_memory': False, 'second_order': False, 'n_memories': 0, 'memory_strength': 0, 'steps_per_sample': 1, 'gamma': 1.0, 'beta': 1.0, 's': 1, 'batches_per_example': 1, 'bgd_optimizer': 'bgd', 'optimizer_params': '{}', 'train_mc_iters': 5, 'std_init': 0.05, 'mean_eta': 1, 'fisher_gamma': 0.95, 'rln': 7, 'update_steps': 10, 'meta_lr': 0.001, 'update_lr': 0.1, 'ctn_lr': 0.01, 'ctn_beta': 0.05, 'ctn_n_meta': 2, 'ctn_inner_steps': 2, 'ctn_temperature': 5, 'ctn_n_memories': 50, 'ctn_alpha_init': 0.1, 'ctn_memory_strength': 0.5, 'ctn_task_emb': 64, 'bcl_n_memories': 2000, 'bcl_memory_strength': 1.0, 'bcl_temperature': 2.0, 'bcl_inner_steps': 5, 'bcl_n_meta': 5, 'bcl_adapt_lr': 0.1} # val: 0.222 0.228 0.007 0.001 # test: 0.222 0.228 0.007 0.000 # 12.988465785980225
[STATE 2025-12-03 10:37:33] Results saved; total runtime 12.99s
New Experiment Starting...
Running model:  eralg4
[STATE 2025-12-03 10:37:36] Experiment 'all_la-eralg4' starting with model 'eralg4' (seed 0)
Set seed 0
Loaded task0.npz: Unique train labels: [2 3 5 7 8]
Loaded task0.npz: Remapped labels: [0 1 2 3 4]
Loaded task1.npz: Unique train labels: [0 1 4 6 9]
Loaded task1.npz: Remapped labels: [5 6 7 8 9]
[STATE 2025-12-03 10:37:38] Loader 'task_incremental_loader' ready: 4096 inputs, 10 outputs, 2 tasks
n_outputs: 10 	n_tasks: 2
[STATE 2025-12-03 10:37:38] Logging to logs//eralg4/all_la-eralg4-2025-12-03_10-37-38-1960/0
True
Model device: cuda:0
[STATE 2025-12-03 10:37:38] Model initialized on device cuda:0
[STATE 2025-12-03 10:37:38] Invoking continual life experience flow
[STATE 2025-12-03 10:37:38] Life experience start: 2 tasks queued
[STATE 2025-12-03 10:37:38] Starting task 0 (1/2)
[STATE 2025-12-03 10:37:38] Task 0 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:37:38] Task 0 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.20436507936507936, 0.1988095238095238] ----
Task 0 Epoch 1/1 | Loss 1.7264 | Train Acc 0.2422 | Val Acc 0.2044 | Epoch Time 2.39s (Eval 1.67s, Train 0.72s)
[STATE 2025-12-03 10:37:40] Task 0 Epoch 1/1 complete: 2.39s total (1.67s eval/0.72s train)
[STATE 2025-12-03 10:37:40] Task 0: running final validation.
[STATE 2025-12-03 10:37:45] Completed task 0 (1/2)
[STATE 2025-12-03 10:37:45] Starting task 1 (2/2)
[STATE 2025-12-03 10:37:45] Task 1 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:37:45] Task 1 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.2492063492063492, 0.2007936507936508] ----
Task 1 Epoch 1/1 | Loss 1.5393 | Train Acc 0.3177 | Val Acc 0.2008 | Epoch Time 2.43s (Eval 1.54s, Train 0.89s)
[STATE 2025-12-03 10:37:47] Task 1 Epoch 1/1 complete: 2.43s total (1.54s eval/0.89s train)
[STATE 2025-12-03 10:37:47] Task 1: running final validation.
[STATE 2025-12-03 10:37:52] Completed task 1 (2/2)
####Final Validation Accuracy####
Final Results:- 
 Total Accuracy: 0.22976190476190478 
 Individual Accuracy: [0.2507936507936508, 0.20873015873015874]
####Final Test Accuracy####
Final Results:- 
 Total Accuracy: 0.22976190476190478 
 Individual Accuracy: [0.2507936507936508, 0.20873015873015874]
[STATE 2025-12-03 10:37:52] Saving results to logs//eralg4/all_la-eralg4-2025-12-03_10-37-38-1960/0/results
logs//eralg4/all_la-eralg4-2025-12-03_10-37-38-1960/0/results: {'expt_name': 'all_la-eralg4', 'model': 'eralg4', 'arch': 'resnet1d', 'n_hiddens': 100, 'n_layers': 2, 'xav_init': False, 'glances': 1, 'n_epochs': 1, 'batch_size': 128, 'replay_batch_size': 64.0, 'memories': 200, 'lr': 0.001, 'cuda': True, 'seed': 0, 'log_every': 3125, 'log_dir': 'logs//eralg4/all_la-eralg4-2025-12-03_10-37-38-1960/0', 'tf_dir': 'logs//eralg4/all_la-eralg4-2025-12-03_10-37-38-1960/0/tfdir', 'calc_test_accuracy': True, 'state_logging': True, 'data_path': 'data/rff/rfmls/', 'loader': 'task_incremental_loader', 'samples_per_task': 128, 'shuffle_tasks': False, 'classes_per_it': 4, 'iterations': 5000, 'dataset': 'iq', 'workers': 3, 'validation': 0.3, 'class_order': 'random', 'increment': 5, 'test_batch_size': 100000, 'nc_per_task': 5, 'val_rate': 10, 'opt_lr': 0.1, 'opt_wt': 0.1, 'alpha_init': 0.1, 'learn_lr': True, 'sync_update': False, 'grad_clip_norm': 2.0, 'cifar_batches': 5, 'use_old_task_memory': False, 'second_order': False, 'n_memories': 0, 'memory_strength': 0, 'steps_per_sample': 1, 'gamma': 1.0, 'beta': 1.0, 's': 1, 'batches_per_example': 1, 'bgd_optimizer': 'bgd', 'optimizer_params': '{}', 'train_mc_iters': 5, 'std_init': 0.05, 'mean_eta': 1, 'fisher_gamma': 0.95, 'rln': 7, 'update_steps': 10, 'meta_lr': 0.001, 'update_lr': 0.1, 'ctn_lr': 0.01, 'ctn_beta': 0.05, 'ctn_n_meta': 2, 'ctn_inner_steps': 2, 'ctn_temperature': 5, 'ctn_n_memories': 50, 'ctn_alpha_init': 0.1, 'ctn_memory_strength': 0.5, 'ctn_task_emb': 64, 'bcl_n_memories': 2000, 'bcl_memory_strength': 1.0, 'bcl_temperature': 2.0, 'bcl_inner_steps': 5, 'bcl_n_meta': 5, 'bcl_adapt_lr': 0.1} # val: 0.229 0.230 0.001 0.001 # test: 0.229 0.230 0.001 0.000 # 14.10288143157959
[STATE 2025-12-03 10:37:52] Results saved; total runtime 14.10s
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.
New Experiment Starting...
Running model:  gem
[STATE 2025-12-03 10:37:55] Experiment 'all_gem' starting with model 'gem' (seed 0)
Set seed 0
Loaded task0.npz: Unique train labels: [2 3 5 7 8]
Loaded task0.npz: Remapped labels: [0 1 2 3 4]
Loaded task1.npz: Unique train labels: [0 1 4 6 9]
Loaded task1.npz: Remapped labels: [5 6 7 8 9]
[STATE 2025-12-03 10:37:57] Loader 'task_incremental_loader' ready: 4096 inputs, 10 outputs, 2 tasks
n_outputs: 10 	n_tasks: 2
[STATE 2025-12-03 10:37:57] Logging to logs//gem/all_gem-2025-12-03_10-37-57-2904/0
True
Model device: cuda:0
[STATE 2025-12-03 10:37:57] Model initialized on device cuda:0
[STATE 2025-12-03 10:37:57] Invoking continual life experience flow
[STATE 2025-12-03 10:37:57] Life experience start: 2 tasks queued
[STATE 2025-12-03 10:37:57] Starting task 0 (1/2)
[STATE 2025-12-03 10:37:57] Task 0 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:37:57] Task 0 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.046825396825396826, 0.15912698412698412] ----
Task 0 Epoch 1/1 | Loss 1.7264 | Train Acc 0.2422 | Val Acc 0.0468 | Epoch Time 1.86s (Eval 1.67s, Train 0.18s)
[STATE 2025-12-03 10:37:59] Task 0 Epoch 1/1 complete: 1.86s total (1.67s eval/0.18s train)
[STATE 2025-12-03 10:37:59] Task 0: running final validation.
[STATE 2025-12-03 10:38:03] Completed task 0 (1/2)
[STATE 2025-12-03 10:38:03] Starting task 1 (2/2)
[STATE 2025-12-03 10:38:03] Task 1 Epoch 1/1: entering train loop
[STATE 2025-12-03 10:38:03] Task 1 Epoch 1/1 Iter 0: running validation
---- Eval at Epoch 0: [0.11587301587301588, 0.10912698412698413] ----
Traceback (most recent call last):
  File "/home/lunet/wsmr11/repos/La-MAML/main.py", line 382, in <module>
    main()
  File "/home/lunet/wsmr11/repos/La-MAML/main.py", line 372, in main
    result_val_t, result_val_a, result_test_t, result_test_a, spent_time = life_experience(
  File "/home/lunet/wsmr11/repos/La-MAML/main.py", line 218, in life_experience
    loss, tr_acc = model.observe(Variable(v_x), Variable(v_y), task_info["task"])
  File "/home/lunet/wsmr11/repos/La-MAML/model/gem.py", line 300, in observe
    ptloss.backward()
  File "/home/lunet/wsmr11/repos/La-MAML/la-maml_env/lib/python3.10/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/lunet/wsmr11/repos/La-MAML/la-maml_env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/lunet/wsmr11/repos/La-MAML/la-maml_env/lib/python3.10/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.AcceleratorError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

